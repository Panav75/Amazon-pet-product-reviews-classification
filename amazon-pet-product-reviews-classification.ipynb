{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"pygments_lexer":"ipython3","nbconvert_exporter":"python","version":"3.6.4","file_extension":".py","codemirror_mode":{"name":"ipython","version":3},"name":"python","mimetype":"text/x-python"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"import numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom tqdm import tqdm\ntqdm.pandas()\n\ntrain_df = pd.read_csv('/kaggle/input/amazon-pet-product-reviews-classification/train.csv')\ntest_df = pd.read_csv('/kaggle/input/amazon-pet-product-reviews-classification/test.csv')\nval_df = pd.read_csv('/kaggle/input/amazon-pet-product-reviews-classification/valid.csv')\n\nunlabeled = pd.read_csv('/kaggle/input/amazon-pet-product-reviews-classification/unlabeled.csv')\nsample_submission = pd.read_csv('/kaggle/input/amazon-pet-product-reviews-classification/sample_submission.csv')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df.shape, test_df.shape, val_df.shape, unlabeled.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels_to_ids = {}\nids_to_labels = {}\nfor i, label in enumerate(sorted(train_df['label'].unique())):\n    labels_to_ids[label] = i\n    ids_to_labels[i] = label\n    \nlabels_to_ids, ids_to_labels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from nltk.corpus import stopwords \nfrom nltk.tokenize import word_tokenize\nfrom nltk.stem.porter import PorterStemmer\nfrom sklearn.feature_extraction.text import TfidfVectorizer\n\nfrom sklearn.metrics import classification_report\n\nfrom sklearn.linear_model import LogisticRegression\n\nimport re","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def generate_features(df_, return_vocab=False):\n    \n    df = df_.copy()\n    stops = stopwords.words('english')\n    stemmer = PorterStemmer()\n    \n    # removing special characters\n    df['prepared_text'] = df['text'].progress_apply(lambda text: re.sub('[^A-Za-z]', ' ', text))\n    # transform text to lowercase\n    df['prepared_text'] = df['prepared_text'].str.lower()\n    # tokenize the texts\n    df['prepared_text'] = df['prepared_text'].progress_apply(lambda text: word_tokenize(text))\n    # removing stopwords\n    df['prepared_text'] = df['prepared_text'].progress_apply(lambda words: [word for word in words if word not in stops])\n    # stemming\n    df['prepared_text'] = df['prepared_text'].progress_apply(lambda words: [stemmer.stem(word) for word in words])\n    \n    # join prepared_+text to use as corpus\n    df['joined_prepared_text'] = df['prepared_text'].progress_apply(lambda words: \" \".join(words))\n    \n    if (return_vocab):\n        vocabulary = set(np.concatenate(train_df['prepared_text'].values))\n        print(f\"There are {len(vocabulary)} words in vocabulary\")\n        \n        return df, vocabulary\n    \n    return df","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train = generate_features(train_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val = generate_features(val_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test = generate_features(test_df)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"corpus = train['joined_prepared_text'].values\ncorpus[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_corpus = val['joined_prepared_text'].values\nval_corpus[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_corpus = test['joined_prepared_text'].values\ntest_corpus[:2]","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"vectorizer = TfidfVectorizer(max_features=10000)\nX = vectorizer.fit_transform(corpus)\nX.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y = train_df['label'].map(labels_to_ids).values","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression(random_state=42, max_iter=200).fit(X, y)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict(X)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"labels = list(ids_to_labels.values())\n\nlabels","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y, y_pred, target_names=labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_val = vectorizer.transform(val_corpus)\nX_val.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_true = val['label'].map(labels_to_ids).values\ny_pred = clf.predict(X_val)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(y_true, y_pred, target_names=labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test.head()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = vectorizer.transform(test_corpus)\nX_test.shape","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"y_pred = clf.predict(X_test)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['label'] = y_pred","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['label'] = sample_submission['label'].map(ids_to_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv('baseline_submission.csv', index=None, header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from transformers import (AutoModelForSequenceClassification, AdamW, \n                          Trainer, TrainingArguments, PreTrainedTokenizerFast,\n                          EarlyStoppingCallback, AutoTokenizer)\n\nimport torch\nfrom torch.utils.data import Dataset\nfrom torch.utils.data.dataloader import DataLoader\n\nfrom sklearn.metrics import classification_report","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df = train_df.sample(1000, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df = val_df.sample(500, random_state=42)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_df['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"val_df['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train = train_df['text'].tolist()\nX_val = val_df['text'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"tokenizer = AutoTokenizer.from_pretrained('bert-base-cased')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tokenized_batch = tokenizer(X_train, truncation=True, max_length=256, padding=True, return_tensors='pt')\nval_tokenized_batch = tokenizer(X_val, truncation=True, max_length=256, padding=True, return_tensors='pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_tokenized_batch['labels'] = train_df['label'].map(labels_to_ids).tolist()\nval_tokenized_batch['labels'] = val_df['label'].map(labels_to_ids).tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class Dataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.tokenized_batch = data\n\n    def __getitem__(self, idx):   \n        return {\n            'input_ids': self.tokenized_batch['input_ids'][idx],\n            'attention_mask': self.tokenized_batch['attention_mask'][idx],\n            'labels': self.tokenized_batch['labels'][idx]\n        }\n\n    def __len__(self):\n        return len(self.tokenized_batch['input_ids'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"train_dataset = Dataset(train_tokenized_batch)\nval_dataset = Dataset(val_tokenized_batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"len(train_dataset), len(val_dataset)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dl_train = DataLoader(train_dataset, batch_size=8, shuffle=False)\ndl_val = DataLoader(val_dataset, batch_size=16, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"model = AutoModelForSequenceClassification.from_pretrained('bert-base-cased', num_labels=len(ids_to_labels.values()))\n_ = model.cuda()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"optimizer = AdamW(model.parameters(), lr=5e-6)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"epochs = 5\nfor _ in tqdm(range(epochs), desc=\"Epoch\"):\n    tr_loss, nb_tr_steps = 0, 0\n    \n    for batch in dl_train:\n        output = model(input_ids=batch['input_ids'].cuda(), attention_mask=batch['attention_mask'].cuda(), labels=batch['labels'].cuda())\n\n        loss = output.loss\n        loss.backward()\n        \n        optimizer.step()\n        optimizer.zero_grad()\n        \n        nb_tr_steps += 1\n        tr_loss += loss.item()\n        \n    print(f\"Train Loss: {tr_loss / nb_tr_steps}\")\n    \n    eval_loss, nb_eval_steps = 0, 0\n    for batch in dl_val:\n        with torch.no_grad():\n            output = model(input_ids=batch['input_ids'].cuda(), attention_mask=batch['attention_mask'].cuda(), labels=batch['labels'].cuda())\n            \n        loss = output.loss\n        \n        eval_loss += loss.item()\n        nb_eval_steps += 1\n        \n    print(f\"Eval loss: {eval_loss / nb_eval_steps}\")","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"training_args = TrainingArguments(\n    output_dir='./results',          # output directory\n    num_train_epochs=20,              # total # of training epochs\n    per_device_train_batch_size=8,  # batch size per device during training\n    per_device_eval_batch_size=16,   # batch size for evaluation\n    evaluation_strategy=\"epoch\",\n    logging_strategy=\"epoch\",\n    prediction_loss_only=True,\n    logging_dir='./logs',            # directory for storing logs\n    seed=42,\n    fp16=True,\n    save_total_limit=1,\n    gradient_accumulation_steps=2,\n    load_best_model_at_end=True,\n    learning_rate=5e-6, \n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer = Trainer(\n    model=model,                         # the instantiated 🤗 Transformers model to be trained\n    args=training_args,                  # training arguments, defined above\n    train_dataset=train_dataset,         # training dataset\n    eval_dataset=val_dataset,            # evaluation dataset\n    callbacks=[EarlyStoppingCallback(early_stopping_patience=5)]\n)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.train()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"best_model = trainer.model","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dl_val = DataLoader(val_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\ntrue_labels = []\nfor batch in tqdm(dl_val):\n    \n    with torch.no_grad():\n        output = best_model(input_ids=batch['input_ids'].cuda(), attention_mask=batch['attention_mask'].cuda())\n    \n    logits = output.logits  \n    val_batch_preds = torch.argmax(output.logits, axis=1).cpu().numpy()\n    predictions.extend(val_batch_preds)\n    true_labels.extend(batch['labels'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print(classification_report(true_labels, predictions, target_names=labels))","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"trainer.evaluate()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_test = test_df['text'].tolist()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"test_tokenized_batch = tokenizer(X_test, return_token_type_ids=False, truncation=True, max_length=512, padding=True, return_tensors='pt')","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"class PredictDataset(torch.utils.data.Dataset):\n    def __init__(self, data):\n        self.tokenized_batch = data\n\n    def __getitem__(self, idx):   \n        return {\n            'input_ids': self.tokenized_batch['input_ids'][idx],\n            'attention_mask': self.tokenized_batch['attention_mask'][idx],\n        }\n\n    def __len__(self):\n        return len(self.tokenized_batch['input_ids'])","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predict_dataset = PredictDataset(test_tokenized_batch)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"dl = DataLoader(predict_dataset, batch_size=64, shuffle=False)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"predictions = []\nfor batch in tqdm(dl):\n    \n    with torch.no_grad():\n        output = best_model(input_ids=batch['input_ids'].cuda(), attention_mask=batch['attention_mask'].cuda())\n    \n    logits = output.logits  \n    val_batch_preds = torch.argmax(output.logits, axis=1).cpu().numpy()\n    predictions.extend(val_batch_preds)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['label'] = predictions","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['label'] = sample_submission['label'].map(ids_to_labels)","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission['label'].value_counts()","metadata":{"trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"sample_submission.to_csv('roberta_submission.csv', index=None, header=True)","metadata":{"trusted":true},"execution_count":null,"outputs":[]}]}