{"metadata":{"kernelspec":{"language":"python","display_name":"Python 3","name":"python3"},"language_info":{"name":"python","version":"3.7.12","mimetype":"text/x-python","codemirror_mode":{"name":"ipython","version":3},"pygments_lexer":"ipython3","nbconvert_exporter":"python","file_extension":".py"}},"nbformat_minor":4,"nbformat":4,"cells":[{"cell_type":"code","source":"# This Python 3 environment comes with many helpful analytics libraries installed\n# It is defined by the kaggle/python Docker image: https://github.com/kaggle/docker-python\n# For example, here's several helpful packages to load\n\nimport numpy as np # linear algebra\nimport pandas as pd # data processing, CSV file I/O (e.g. pd.read_csv)\nfrom __future__ import print_function\nimport matplotlib.pyplot as plt\nimport numpy as np\nimport os\nimport sys\nfrom six.moves import cPickle as pickle\nimport pandas as pd\nimport gzip\nimport seaborn as sns\nimport string\nfrom time import time\nimport nltk\nfrom nltk.corpus import stopwords \nstops = set(stopwords.words(\"english\"))\n\n\nfrom IPython.display import display # Allows the use of display() for DataFrames\n\n\nimport warnings\nwarnings.filterwarnings('ignore')\n\n# Config the matlotlib backend as plotting inline in IPython\n%matplotlib inline\n\n# Lets define some helpful constants\nDATASET_NAME = 'reviews_Apps_for_Android_5.json.gz'\n\n# A size for figures\nFIG_SIZE = (14,8)\n\n#Random state for classifiers\nRAN_STATE = 42\n\n# Input data files are available in the read-only \"../input/\" directory\n# For example, running this (by clicking run or pressing Shift+Enter) will list all files under the input directory\n\nimport os\nfor dirname, _, filenames in os.walk('/kaggle/input'):\n    for filename in filenames:\n        print(os.path.join(dirname, filename))\n\n# You can write up to 20GB to the current directory (/kaggle/working/) that gets preserved as output when you create a version using \"Save & Run All\" \n# You can also write temporary files to /kaggle/temp/, but they won't be saved outside of the current session","metadata":{"_uuid":"8f2839f25d086af736a60e9eeb907d3b93b6e0e5","_cell_guid":"b1076dfc-b9ad-4769-8c92-a6c4dae69d19","trusted":true},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"url = 'http://snap.stanford.edu/data/amazon/productGraph/categoryFiles/'\nlast_percent_reported = None\n\ndef download_progress_hook(count, blockSize, totalSize):\n    \"\"\"A hook to report the progress of a download. This is mostly intended for users with\n    slow internet connections. Reports every 1% change in download progress.\n    \"\"\"\n    global last_percent_reported\n    percent = int(count * blockSize * 100 / totalSize)\n\n    if last_percent_reported != percent:\n        if percent % 5 == 0:\n            sys.stdout.write(\"%s%%\" % percent)\n            sys.stdout.flush()\n        else:\n            sys.stdout.write(\".\")\n            sys.stdout.flush()\n            \n        last_percent_reported = percent\n                \ndef maybe_download(filename, expected_bytes, force=False):\n    \"\"\"Download a file if not present, and make sure it's the right size.\"\"\"\n    if force or not os.path.exists(filename):\n        print('Attempting to download:', filename) \n        filename, _ = urlretrieve(url + filename, filename, reporthook=download_progress_hook)\n        print('\\nDownload Complete!')\n    statinfo = os.stat(filename)\n    if statinfo.st_size == expected_bytes:\n        print('Found and verified', filename)\n    else:\n        raise Exception(\n            'Failed to verify ' + filename + '. Can you get to it with a browser?')\n    return filename\n\ntest = maybe_download(DATASET_NAME, 95509687)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"def parse(path):\n    g = gzip.open(path, 'rb')\n    for l in g:\n        yield eval(l)\n\ndef getDF(path):\n    i = 0\n    data = {}\n    for d in parse(path):\n        data[i] = d\n        i += 1\n    return pd.DataFrame.from_dict(df, orient='index')\n\n#removes extensions and adds pickle.\npickle_file_name = (DATASET_NAME[:-8]+'.pickle')\n\n#loads pickle if exists, extracts and pickles if it doesn't\nif os.path.exists(pickle_file_name):\n    print ('Pickled file already present, loading...')\n    data = pd.read_pickle(pickle_file_name)\n    print ('Pickle file loaded.')\nelse:\n    data = getDF(DATASET_NAME)\n    data.to_pickle(pickle_file_name)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"data.head(1)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#select the columns\ndf = data.iloc[:, [6,5,3]]\n\n#split numerator and denominator\ndf['helpful_numerator'] = df['helpful'].apply(lambda x: x[0])\ndf['helpful_denominator'] = df['helpful'].apply(lambda x: x[1])\n\n# delete un-needed 'helpful catagory\ndel df['helpful']\n\n#Check if we have any null values\nprint (df.isnull().sum())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.describe()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Box Plot\nplt.figure(figsize=FIG_SIZE)\nplt.title('Box plot of Features')\nplt.ylabel('Spread')\nplt.xlabel('Features')\n\ndisplay(sns.boxplot(df[df.columns]))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"print (df['helpful_numerator'].idxmax(axis=0, skipna=True))\nprint (df['helpful_denominator'].idxmax(axis=0, skipna=True))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"df.iloc[[510127,510128]]","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#include reviews that have more than 10 helpfulness data point only\ndf1 = df[(df.helpful_denominator > 10)].copy()\ndf1.shape","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#transform Helpfulness into a binary variable with 0.50 ratio\nthreshold = 0.5\ndf1.loc[:, 'Helpful'] = np.where(df1.loc[:, 'helpful_numerator'] \\\n                                 / df1.loc[:, 'helpful_denominator'] > threshold, 1, 0)\ndf1.head(3)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Check the balance\nprint ('Count:')\ndisplay(df1.groupby('Helpful').count())","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#Visualize correlation of the data\ncorrelations = df1.corr()\nplt.figure(figsize = FIG_SIZE)\nplt.title(\"Heatmap of correlations in each catagory\")\n_ = sns.heatmap(correlations, vmin=0, vmax=1, annot=True)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# convert text to lowercase\ndf1.loc[:, 'reviewText'] = df1['reviewText'].str.lower()\n\ndef remove_punctuation(text):\n    return text.translate(None, string.punctuation.translate(None, '\"'))\n\ndf1['reviewText']=df1['reviewText'].apply( lambda x: remove_punctuation(x))\ndf1['reviewText'].head(4)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#tokenize text with Tfidf\nfrom sklearn.feature_extraction.text import TfidfVectorizer\nfrom nltk.stem.snowball import SnowballStemmer\n\n#create a stemmer\nstemmer = SnowballStemmer(\"english\")\n\n\n#define our own tokenizing function that we will pass into the TFIDFVectorizer. We will also stem the words here.\ndef tokens(x):\n    x = x.split()\n    stems = []\n    [stems.append(stemmer.stem(word)) for word in x]\n    return stems\n\n#loads pickle if exists, extracts and pickles if it doesn't\nif os.path.exists('features.pickle'):\n    print ('Pickled file already present, loading...')\n    features = pickle.load( open( \"features.pickle\", \"rb\" ) )\n    print ('Pickle file loaded.')\nelse:\n    #define the vectorizer\n    vectorizer = TfidfVectorizer(tokenizer = tokens, stop_words = 'english', ngram_range=(1, 1), min_df = 0.01)\n    #fit the vectorizers to the data.\n    features = vectorizer.fit_transform(df1['reviewText'])\nfeatures","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.cross_validation import train_test_split\nX_train, X_test, y_train, y_test = train_test_split(features,df1['Helpful'], test_size=0.2, random_state=RAN_STATE)\n","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn.metrics import roc_auc_score, roc_curve\n\ndef train_classifier(clf, X_train, y_train):\n    ''' Fits a classifier to the training data. '''\n    \n    # Start the clock, train the classifier, then stop the clock\n    start = time()\n    clf.fit(X_train, y_train)\n    end = time()\n    \n    # Print the results\n    print (\"Trained model in {:.4f} seconds\".format(end - start))\n\n    \ndef predict_labels(clf, features, target):\n    ''' Makes predictions using a fit classifier based on roc_auc score. '''\n    \n    # Start the clock, make predictions, then stop the clock\n    start = time()\n    probas = clf.predict_proba(features)\n    end = time()\n    \n    # Print and return results\n    print (\"Made predictions in {:.4f} seconds.\".format(end - start))\n    return roc_auc_score(target.values, probas[:,1].T)\n\n\ndef train_predict(clf, X_train, y_train, X_test, y_test):\n    ''' Train and predict using a classifer based on roc_auc score. '''\n    \n    # Indicate the classifier and the training set size\n    print (\"Training a {} using a training set size of {}. . .\".format(clf.__class__.__name__, X_train.shape[0]))\n    \n    # Train the classifier\n    train_classifier(clf, X_train, y_train)\n    \n    # Print the results of prediction for both training and testing\n    print (\"ROC_AUC score for training set: {:.4f}.\".format(predict_labels(clf, X_train, y_train)))\n    print (\"ROC_AUC score for test set: {:.4f}.\\n\".format(predict_labels(clf, X_test, y_test)))\n    \ndef clf_test_roc_score(clf, X_train, y_train, X_test, y_test):\n    clf.fit(X_train, y_train)\n    probas = probas =clf.predict_proba(X_test)\n    return roc_auc_score(y_test, probas[:,1].T)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"# Import the supervised learning models from sklearn\nfrom sklearn.naive_bayes import GaussianNB\nfrom sklearn.ensemble import AdaBoostClassifier\nfrom sklearn.ensemble import RandomForestClassifier\nfrom sklearn.linear_model import LogisticRegression\nfrom sklearn.tree import DecisionTreeClassifier\n\n\n# Initialize the models using a random state were applicable.\nclf_list = [GaussianNB(), \n            AdaBoostClassifier(random_state = RAN_STATE), \n            RandomForestClassifier(random_state = RAN_STATE), \n            LogisticRegression(random_state = RAN_STATE),\n            DecisionTreeClassifier(random_state = RAN_STATE)]\nx_tr = X_train.toarray()\nx_te = X_test.toarray()\n\n\n# Set up the training set sizes for 100, 200 and 300 respectively.\ntrain_feature_list = [x_tr[0:10000],x_tr[0:20000],x_tr]\ntrain_target_list = [y_train[0:10000], y_train[0:20000], y_train]\n\n\n# Execute the 'train_predict' function for each of the classifiers and each training set size\nfor clf in clf_list:\n    for a, b in zip(train_feature_list, train_target_list):\n        train_predict(clf, a, b, x_te, y_test)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"### Visualize all of the classifiers                                                               \nfor clf in clf_list:\n    x_graph = []\n    y_graph = []\n    for a, b in zip(train_feature_list, train_target_list):\n        y_graph.append(clf_test_roc_score(clf, a, b, x_te, y_test))\n        x_graph.append(len(a))\n    plt.scatter(x_graph,y_graph)\n    plt.plot(x_graph,y_graph, label = clf.__class__.__name__)\n\nplt.title('Comparison of Different Classifiers')\nplt.xlabel('Training Size')\nplt.ylabel('ROC_AUC score on test set')\nplt.legend(bbox_to_anchor=(1.6, 1.05))\nplt.figure(figsize=FIG_SIZE)             \nplt.show()","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#add Score column to features\nimport scipy as scipy\n\nfeatures = pickle.load( open( \"features.pickle\", \"rb\" ) )\noverall = np.array(list(df1.overall))\noverall = overall.reshape(features.shape[0], 1)\n\nfeatures = scipy.sparse.hstack((features,scipy.sparse.csr_matrix(overall)))\n\nfeatures = scipy.sparse.csr_matrix(features)\nfeatures","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"X_train2, X_test2, y_train, y_test = train_test_split(features, df1['Helpful'], test_size=0.2, random_state=RAN_STATE)","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"from sklearn import grid_search, cross_validation\n#make the grid search object\ngs2 = grid_search.GridSearchCV(\n    estimator=LogisticRegression(),\n    param_grid={'C': [10**i for i in range(-5,5)], 'class_weight': [None, 'balanced']},\n    cv=cross_validation.StratifiedKFold(y_train,n_folds=5),\n    scoring='roc_auc'\n)\n\n#fit the grid search object to our new dataset\nprint ('Fitting grid search...')\ngs2.fit(X_train2, y_train)\nprint (\"Grid search fitted.\")","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"#print the grid search scores.\nprint (gs2.best_estimator_)\ngs2.grid_scores_","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf2 = gs2.best_estimator_\nprobas =clf2.predict_proba(X_test2)\nplt.figure(figsize = FIG_SIZE)\nplt.plot(roc_curve(y_test, probas[:,1])[0], roc_curve(y_test, probas[:,1])[1])\nplt.title('ROC Curve for Helpful Rating')\nplt.grid()\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\nplt.show\n\n# ROC/AUC score\nprint ('ROC_AUC Score:',roc_auc_score(y_test, probas[:,1].T))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"random_numbers = range(1,101)\ntot = 0.0\nfor seed in random_numbers:\n    clf3 = LogisticRegression(random_state=seed)\n    clf3.fit(X_train2, y_train)\n    probas =clf3.predict_proba(X_test2)\n    tot += roc_auc_score(y_test, probas[:,1].T)\n    \nprint ('Average ROC_AUC Score for 1-100 random_state: {:.4f}'.format(tot/100))","metadata":{},"execution_count":null,"outputs":[]},{"cell_type":"code","source":"clf = LogisticRegression()\nclf.fit(X_train,y_train)\nprobas = clf.predict_proba(X_test)\nclf2 = gs2.best_estimator_\nprobas2 =clf2.predict_proba(X_test2)\nplt.figure(figsize = FIG_SIZE)\n\nplt.plot(roc_curve(y_test, probas[:,1])[0], roc_curve(y_test, probas[:,1])[1], label = 'TFIDF')\nplt.plot(roc_curve(y_test, probas2[:,1])[0], roc_curve(y_test, probas2[:,1])[1], label = 'TFIDF + overall')\nplt.title('ROC Curve for Helpful Rating')\nplt.grid()\nplt.xlabel('False Positive Rate (1 - Specificity)')\nplt.ylabel('True Positive Rate (Sensitivity)')\n\nplt.legend(bbox_to_anchor=(1.0, .5))\nplt.figure(figsize=FIG_SIZE) \nplt.show()","metadata":{},"execution_count":null,"outputs":[]}]}